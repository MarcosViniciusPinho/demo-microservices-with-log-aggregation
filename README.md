# Implementing the log aggregation pattern within microservices

When it comes to microservices, applying the log aggregation pattern is crucial. This pattern solves the problem of having multiple logs scattered throughout the microservices ecosystem by centralizing them in one place, making it easier to monitor our applications.

To get started, it's essential to have Docker and Docker Compose installed on your machine. If you don't already have them, please install them before proceeding.

Now, let's take a closer look at the project. It's divided into several microservices, each with log aggregation capabilities, as illustrated in the image below:
<br />![](images/microservices_division.png)

The image displays three available microservices: demoA-api, demoB-api, and demoC-api.

demoA-api is behind a gateway that can be accessed externally and forwards requests to the demoB-api microservice. demoB-api, on the other hand, is only visible within the internal network and passes requests to demoC-api, which retrieves the appropriate response and sends it back to demoA-api. demoC-api is also only accessible on the internal network and sends responses to demoB-api.

To implement the log aggregation pattern and monitor microservices logs, we adopted ELKB(elasticstack)*. This solution was divided into several stages, including capturing logs, processing them, and visualizing them, as illustrated in the image below:
![](images/elasticstack_division.png)

In the image provided, we can observe the availability of four docker services(EKLB): Filebeat, Logstash, Elasticsearch, and Kibana.

Filebeat is responsible for monitoring logs from containers on the same network and forwarding them to Logstash for processing. Logstash, in turn, receives the logs from Filebeat and processes them, focusing solely on the information contained in our microservices. The processed logs are then sent to Elasticsearch for storage and indexing.

Elasticsearch receives, stores, and indexes the logs, and provides a visual representation of them. It also allows us to create queries based on the logs. Kibana is another tool for visualizing the logs stored in Elasticsearch. It can be used as a dashboard, and allows us to create queries to retrieve specific information of interest.

**Important considerations:**
<ul>
    <li>The primary objective of this project is to showcase the application of the log aggregation pattern. Therefore, the focus is not on the microservices themselves, as they are relatively simple.</li>
    <li>The project has been designed to operate on any operating system with docker and docker-compose installed.</li>
    <li>While there may be alternative configurations and approaches, the intention here is to keep things as straightforward as possible, to facilitate a better understanding of the log aggregation pattern in both theory and practice.</li>
</ul>

Alright, it's time to get started!

Here's a step-by-step guide to help you through the process:

1) First, we need to start all of our microservices, as well as our log aggregation service. To do this, simply type the following command in your terminal: `docker-compose up -d`
2) Wait a few seconds for the solution to fully deploy. Once the environment is available, access the following URL: 
[http://localhost:5601/login?next=%2F]()
3) You will see the login screen, as shown below:
<br />![](images/login_elastic.png)

Note: The access credentials for the default user are as follows:
- Username: elastic
- Password: changeme

4) After successfully logging in, navigate to the side menu, select "Analytics", and then click on "Discover". Follow the image below for guidance:
![](images/selected_discover_create_index.png)

5) Upon clicking "Discover", a modal window will appear for creating an index pattern. Click on the "Create index pattern" button, and you will be redirected to the following screen, as shown in the image below:
![](images/create_index_pattern.png)

Here are the steps to follow, depending on the screen:

- In the first field, we must enter the index pattern that matches the names of the log files we want to analyze. We can use the wildcard symbol * to match any characters in the index pattern. For example, if we want to analyze all log files generated by Filebeat, we can use the index pattern `filebeat-*`. If we want to analyze logs from a specific month and year, we can use a more specific index pattern like `filebeat-7.16.3-2023.03.*`.
- In the second field, we must select `@timestamp`, which is the timestamp field in the log entries that we want to use as the time filter for our analysis. This field indicates when each log entry was created and is commonly used to filter logs by time.

6) To generate some logs, let's run the following command in the terminal: `curl --location --request GET 'http://localhost:8080'`

7) After creating the index, we need to navigate back to the "Discover" menu, where we can access the logs, as illustrated in the image below:
![](images/detail_logs.png)
